
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>word2vec using numpy &#8212; TTS Research Technology Guides</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/style/bugfix.css?v=736ec69d" />
    <link rel="stylesheet" type="text/css" href="../../_static/style/footer.css?v=5f497d55" />
    <link rel="stylesheet" type="text/css" href="../../_static/style/gallery.css?v=c2bd73dd" />
    <link rel="stylesheet" type="text/css" href="../../_static/style/navbar.css?v=147f7454" />
    <link rel="stylesheet" type="text/css" href="../../_static/style/sidebar.css?v=ff9f960e" />
    <link rel="stylesheet" type="text/css" href="../../_static/style/switcher.css?v=3e40d84b" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../_static/documentation_options.js?v=8313637b"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-ZJ5LXFRJ2G"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-ZJ5LXFRJ2G');
            </script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/deep-learning/w2v-from-scratch';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.0';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/TuftsRT/guides/refs/heads/switcher/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.0.0-dev';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../../_static/script/dynamic-nav-dropdown.js?v=5e71fbcf"></script>
    <link rel="canonical" href="https://rtguides.it.tufts.edu/nlp/deep-learning/w2v-from-scratch.html" />
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Natural Language Processing: Using Word Vectors for Textual Analysis" href="w2v-gensim.html" />
    <link rel="prev" title="Byte-Pair Encoding Tokenization" href="tokenizers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="20251110" />
    <meta name="docbuild:last-update" content="Nov 10, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder=""
         aria-label=""
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
  <aside class="bd-header-announcement d-print-none d-none" aria-label="Announcement" data-pst-announcement-url="https://raw.githubusercontent.com/TuftsRT/guides/refs/heads/announcement/announcement.html"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">     
<a class="navbar-brand logo" href="../../index.html">
           
  <img
    src="../../_static/jumbo.png"
    class="logo__image only-light"
    alt=""
  />
  <img
    src="../../_static/jumbo.png"
    class="logo__image only-dark pst-js-only"
    alt=""
  />
   
  <p class="title logo__title" id="long_title">
    TTS Research Technology Guides
  </p>
  
  <p class="title logo__title" id="short_title">TTS RT Guides</p>
   
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    Natural Language Processing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../hpc/index.html">
    High-Performance Computing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../bio/index.html">
    Bioinformatics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../viz/index.html">
    Data Visualization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://go.tufts.edu/geospatial">
    Geospatial
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="https://rtguides.it.tufts.edu/dev/tags/index.html" title="Tags" class="nav-link pst-navbar-icon" rel="noopener" target="_self" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-tags fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Tags</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/TuftsRT/guides" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="mailto:tts-research@tufts.edu" title="Email" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Email</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    Natural Language Processing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../hpc/index.html">
    High-Performance Computing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../bio/index.html">
    Bioinformatics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../viz/index.html">
    Data Visualization
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://go.tufts.edu/geospatial">
    Geospatial
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="https://rtguides.it.tufts.edu/dev/tags/index.html" title="Tags" class="nav-link pst-navbar-icon" rel="noopener" target="_self" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-tags fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Tags</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/TuftsRT/guides" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="mailto:tts-research@tufts.edu" title="Email" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Email</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">
     Natural Language Processing 
  </p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../text-proc/index.html">Text Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../webscraping/index.html">Webscraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained-models/index.html">Using Pretrained Models</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Deep Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="decoder-pytorch.html">Decoder-only Transformers: Generative Pre-trained Transformers (GPTs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="encoder-pytorch.html">Encoder-only Transformers: Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ner-spacy.html">Extracting Ancient Placenames using Named Entity Recognition and <code class="docutils literal notranslate"><span class="pre">spaCy</span></code></a></li>



<li class="toctree-l2"><a class="reference internal" href="rnn-pytorch.html">Machine Translation (mostly) from Scratch using <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code></a></li>

<li class="toctree-l2"><a class="reference internal" href="tokenizers.html">Byte-Pair Encoding Tokenization</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">word2vec using <code class="docutils literal notranslate"><span class="pre">numpy</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="w2v-gensim.html">Natural Language Processing: Using Word Vectors for Textual Analysis</a></li>

</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../llms/index.html">Large Language Models</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"> 

<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a
        href="../../index.html"
        class="nav-link"
        aria-label="Home"
      >
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
     
    <li class="breadcrumb-item">
      <a href="../index.html" class="nav-link"
        >Natural Language Processing</a
      >
    </li>
     
    <li class="breadcrumb-item">
      <a href="index.html" class="nav-link"
        >Deep Learning</a
      >
    </li>
     
    <li class="breadcrumb-item active" aria-current="page">
      word2vec using <code...
    </li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">bash</span>
<span class="n">DATASETS_DIR</span><span class="o">=</span><span class="s2">&quot;utils/datasets&quot;</span>
<span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="err">$</span><span class="n">DATASETS_DIR</span>
<span class="n">cd</span> <span class="err">$</span><span class="n">DATASETS_DIR</span>

<span class="c1"># Get Stanford Sentiment Treebank</span>
<span class="k">if</span> <span class="nb">hash</span> <span class="n">wget</span> <span class="mi">2</span><span class="o">&gt;/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span><span class="p">;</span> <span class="n">then</span>
  <span class="n">wget</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">nlp</span><span class="o">.</span><span class="n">stanford</span><span class="o">.</span><span class="n">edu</span><span class="o">/~</span><span class="n">socherr</span><span class="o">/</span><span class="n">stanfordSentimentTreebank</span><span class="o">.</span><span class="n">zip</span>
<span class="k">else</span>
  <span class="n">curl</span> <span class="o">-</span><span class="n">L</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">nlp</span><span class="o">.</span><span class="n">stanford</span><span class="o">.</span><span class="n">edu</span><span class="o">/~</span><span class="n">socherr</span><span class="o">/</span><span class="n">stanfordSentimentTreebank</span><span class="o">.</span><span class="n">zip</span> <span class="o">-</span><span class="n">o</span> <span class="n">stanfordSentimentTreebank</span><span class="o">.</span><span class="n">zip</span>
<span class="n">fi</span>
<span class="n">unzip</span> <span class="n">stanfordSentimentTreebank</span><span class="o">.</span><span class="n">zip</span>
<span class="n">rm</span> <span class="n">stanfordSentimentTreebank</span><span class="o">.</span><span class="n">zip</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="word2vec-using-numpy">
<h1>word2vec using <code class="docutils literal notranslate"><span class="pre">numpy</span></code><a class="headerlink" href="#word2vec-using-numpy" title="Link to this heading">#</a></h1>
<p>In this notebook, I’ll walk you through how to implement the popular and flexible word2vec model using <code class="docutils literal notranslate"><span class="pre">numpy</span></code>.</p>
<p>As well as showing how word2vec works, I also want to present to you a workflow that facilitates experimentation and ease of use that we’ll carry through into all of our lessons on neural nets and deep learning. There are off-the-shelf implementations of this algorithm, but writing our own will help us better understand it, <code class="docutils literal notranslate"><span class="pre">numpy</span></code> and Python programming more generally.</p>
<ul class="simple">
<li><p>First, we’ll need to work with the unique challenges of text data. We will develop some Python helper objects that will help us to convert our textual sources into tensors which can be mathematically manipulated.</p></li>
<li><p>Next, we’ll examine the word2vec Continuous Bag of Words (CBoW) and Skipgram architectures and how they work. In doing, we’ll build out the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> modules needed to turn these architectures from theory to practice.</p></li>
<li><p>Last, we’ll create a standard training loop which we can alter and experiment with.</p></li>
</ul>
<p>The learning objectives for this notebook are as follows:</p>
<ul class="simple">
<li><p>Understand the advantages and disadvantages to neural approaches in NLP, and how this relates to our previous use of word2vec.</p></li>
<li><p>Train and interpret their own word2vec model using a non-English language.</p></li>
<li><p>Study the basics of the mathematical underpinnings of deep learning including backpropagation.</p></li>
<li><p>Learn the ways to efficiently run deep learning models.</p></li>
</ul>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>Before we dive into how it works, let’s first take a look at what the goal of word2vec is. As the name implies, this very simple neural net seeks to transform words into numbers. A <strong>vector</strong>, in this sense, refers to a list of numbers whose values represent the meaning of a given word.</p>
<p>This probably sounds a little funky… Why do we need to convert words we know the meaning of into list of numbers whose meaning is hard to grasp? Ultimately, we want to give our computer a way of understanding text it hasn’t seen before and unlike us a computer can’t use text to learn meaning. <em>It can only use numbers</em>. What we really want is a some black-box that we can give a word and it will spit out the meaning of that word to the computer, a list of numbers. In fact, we want to <em>model</em> word meaning. This is what word2vec does.</p>
<p>As we will see, each unique word in our text will have an associated vector attached to it. This vector can be manipulated like any other vector, allowing us to apply complex mathematical operations to word meaning and sense.</p>
<p>In deep learning, this is called <em>feature extraction</em> because we are teaching a model how to extract the linguistic features from a word (though it could be anything, including images or audio). Feature extraction is rarely the end point in analysis. Instead, we can use these extracted features as the inputs to another model which will do some analysis. Before we get there though, let’s look at how we can harness the power of word2vec.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<p>For this example, we will be using the Stanford Sentiment Treebank. This dataset has a lot of excerpts from English newspapers. They have been marked for sentiment value by human annotators, but we won’t be using that data for this lesson. Let’s take a look at what some of the data looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;utils/datasets/stanfordSentimentTreebank&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/datasetSentences.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>

<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[:</span><span class="mi">6</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>These sources are in English (obviously), but the word2vec method is not tied to a particular language or dialect. What makes this framework so successful is that it is flexible and not language dependent. To that end, you will be apply this framework to a non-English language of your choice for your assignment.</p>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h3>
<p>We need to do some processing to make this sentences usable and then we need to do some tokenization. We’ve talked a bit about tokenization before, but in this case we need to do it ourselves. We’ll also need to skip the first line because it is just the column headings.</p>
<p>In the data below, we used “whitespace tokenization”, that is, we split the sentence up on the space character (the default value for <code class="docutils literal notranslate"><span class="pre">.split</span></code> is <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">&quot;</span></code>). This strategy is very easy and works well for English, but may not work as well for other languages. We’ll revisit multi-lingual tokenization later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before Tokenization: &quot;</span><span class="p">,</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;After Tokenization: &quot;</span><span class="p">,</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>
<span class="p">)</span>  <span class="c1"># &quot;whitespace tokenization&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;utils/datasets/stanfordSentimentTreebank&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_sentences</span><span class="p">():</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/datasetSentences.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">first</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">first</span><span class="p">:</span>
                <span class="n">first</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">continue</span>
            <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">sentences</span> <span class="o">+=</span> <span class="p">[[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">split</span><span class="p">]]</span>
    <span class="n">sent_lens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">])</span>
    <span class="n">cum_sent_lens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sent_lens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">sent_lens</span><span class="p">,</span> <span class="n">cum_sent_lens</span>


<span class="n">sentences</span><span class="p">,</span> <span class="n">sent_lens</span><span class="p">,</span> <span class="n">cum_sent_lens</span> <span class="o">=</span> <span class="n">get_sentences</span><span class="p">()</span>

<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># a list of list of strings (word/tokens)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-our-training-dataset-collation">
<h3>Creating our training dataset: Collation<a class="headerlink" href="#creating-our-training-dataset-collation" title="Link to this heading">#</a></h3>
<p>We have done a good job in getting our data from the downloaded files and then tokenizing them, but unfortunately our dataset is far from usable to train a model. As I mentioned above, computers can’t work directly with string or text data. Instead we will somehow need to convert our text into numbers, and not just that, we’ll also need to arrange these numbers so that all of the lists of words are the same length. This means that even though we have a bunch of different sentences with different sizes, we need to standardize them to a single size. This process is called <strong>collation</strong>.</p>
<p>This size experts call <code class="docutils literal notranslate"><span class="pre">block_size</span></code> or <code class="docutils literal notranslate"><span class="pre">context_size</span></code> or <code class="docutils literal notranslate"><span class="pre">context_window</span></code>. You can think about it as the memory of the model. When the model is making predictions about what word will come next, it will only be able to use this context. As a result, we want <code class="docutils literal notranslate"><span class="pre">block_size</span></code> to be as high as possible, but we are limited by computational resources.</p>
<p>We will choose an arbitrary word as a <em>center</em> word. Then get all <code class="docutils literal notranslate"><span class="pre">block_size</span></code> words before and after the center word. From the <code class="docutils literal notranslate"><span class="pre">block_size</span></code> context, we want our model to predict the center word. This is how the model will learn (more on this in a bit). We will end up comparing the correct center word and the predicted center word and adjusting our model according to how similar or dissimilar they are.</p>
<p>Below is a diagram of this process using a <code class="docutils literal notranslate"><span class="pre">block_size</span></code> size of 2 (taken from: <a class="reference external" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a>).</p>
<p><img alt="training_data.png" src="data:image/png;base64,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" /></p>
<p>So first, we need to standardize our sentences so that they all have the same context size. We will set a center token and then retrieve C words before and after it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_random_context</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">sent_id</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sent</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span>
    <span class="n">word_id</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">context</span> <span class="o">=</span> <span class="n">sent</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">word_id</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span> <span class="p">:</span> <span class="n">word_id</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">word_id</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">):</span>
        <span class="n">context</span> <span class="o">+=</span> <span class="n">sent</span><span class="p">[</span><span class="n">word_id</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">),</span> <span class="n">word_id</span> <span class="o">+</span> <span class="n">C</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="n">center</span> <span class="o">=</span> <span class="n">sent</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>
    <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">center</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">center</span><span class="p">,</span> <span class="n">context</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_random_context</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">center</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="n">get_random_context</span><span class="p">()</span>
<span class="n">center</span><span class="p">,</span> <span class="n">context</span>  <span class="c1"># we will use the context words to predict the center word</span>
</pre></div>
</div>
</div>
</div>
<p>“But,” you may be asking yourself, “this doesn’t solve the problem about computers not being able to read text at all! We still have strings! What gives, Peter &gt;:(”</p>
<p>Good point! That part is a bit easier: Because our center word is picked randomly, we can assign each word in our text a number somewhat at random. In fact, we will go sequentially and assign a <em>token id</em> to each word that we haven’t seen before. We will create two dictionaries: one with keys that are each word and values are their token id, and another with keys that are the token ids and the values are their words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_tokens</span><span class="p">():</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tok_freq</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">word_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">rev_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
            <span class="n">word_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">tokens</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">rev_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">tokens</span><span class="p">[</span><span class="s2">&quot;UNK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
    <span class="n">rev_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;UNK&quot;</span><span class="p">]</span>
    <span class="n">tok_freq</span><span class="p">[</span><span class="s2">&quot;UNK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">word_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tok_freq</span><span class="p">,</span> <span class="n">rev_tokens</span><span class="p">,</span> <span class="n">word_count</span>


<span class="n">tokens</span><span class="p">,</span> <span class="n">tok_freq</span><span class="p">,</span> <span class="n">rev_tokens</span><span class="p">,</span> <span class="n">word_count</span> <span class="o">=</span> <span class="n">get_tokens</span><span class="p">()</span>
<span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">tok_freq</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">rev_tokens</span><span class="p">),</span> <span class="n">word_count</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Center word: *&quot;</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="s2">&quot;* Center word token id: &quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="n">center</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">context</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;Context word: *&quot;</span><span class="p">,</span> <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s2">&quot;* Context word token id: &quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="negative-sampling">
<h3>Negative Sampling<a class="headerlink" href="#negative-sampling" title="Link to this heading">#</a></h3>
<p>We’ll come back to this if we have time</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the data came with some splits in our data</span>
<span class="c1"># we can apply them with this function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">dataset_split</span><span class="p">():</span>
    <span class="n">split</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/datasetSplit.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">first</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">first</span><span class="p">:</span>
                <span class="n">first</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">continue</span>
            <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
            <span class="n">split</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">split</span>


<span class="n">split</span> <span class="o">=</span> <span class="n">dataset_split</span><span class="p">()</span>
<span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">table_size</span> <span class="o">=</span> <span class="mf">1e8</span>


<span class="k">def</span><span class="w"> </span><span class="nf">sampleTable</span><span class="p">():</span>
    <span class="n">tokens_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">sampling_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tokens_num</span><span class="p">,))</span>

    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tokens_num</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">rev_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tok_freq</span><span class="p">:</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="n">freq</span><span class="o">**</span><span class="mf">0.75</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">sampling_freq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">sampling_freq</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampling_freq</span><span class="p">)</span>
    <span class="n">sampling_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sampling_freq</span><span class="p">)</span> <span class="o">*</span> <span class="n">table_size</span>

    <span class="n">sample_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">table_size</span><span class="p">),))</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">table_size</span><span class="p">)):</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">sampling_freq</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">sample_table</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>

    <span class="k">return</span> <span class="n">sample_table</span>


<span class="n">sample_table</span> <span class="o">=</span> <span class="n">sampleTable</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reject_prob</span><span class="p">():</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-5</span> <span class="o">*</span> <span class="n">word_count</span>
    <span class="n">reject_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">rev_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
        <span class="n">reject_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">threshold</span> <span class="o">/</span> <span class="n">freq</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">reject_prob</span>


<span class="n">reject_prob</span> <span class="o">=</span> <span class="n">reject_prob</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="our-complete-dataset-object">
<h3>Our complete dataset object<a class="headerlink" href="#our-complete-dataset-object" title="Link to this heading">#</a></h3>
<p>Now that we have coded out the data specific functions, we can compile it all into a single class from which we can call these functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">StanfordSentiment</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for reading and loading Stanford Sentiment Treebank. We ignore the sentiment component of the treebank and extract just the text.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">table_size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;utils/datasets/stanfordSentimentTreebank&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">table_size</span> <span class="o">=</span> <span class="n">table_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">get_sentences</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_tokens</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_all_sentences</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset_split</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampleTable</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tokens&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">tok_freq</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">word_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">rev_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sentences</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
                <span class="n">word_count</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                    <span class="n">tokens</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                    <span class="n">rev_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="n">w</span><span class="p">]</span>
                    <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">tokens</span><span class="p">[</span><span class="s2">&quot;UNK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">rev_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;UNK&quot;</span><span class="p">]</span>
        <span class="n">tok_freq</span><span class="p">[</span><span class="s2">&quot;UNK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">word_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_freq</span> <span class="o">=</span> <span class="n">tok_freq</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rev_tokens</span> <span class="o">=</span> <span class="n">rev_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_count</span> <span class="o">=</span> <span class="n">word_count</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;sentences&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">sentences</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sentences</span>

        <span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">/datasetSentences.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">first</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">first</span><span class="p">:</span>
                    <span class="n">first</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">continue</span>
                <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>
                <span class="n">sentences</span> <span class="o">+=</span> <span class="p">[[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">split</span><span class="p">]]</span>
        <span class="n">sent_lens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">])</span>
        <span class="n">cum_sent_lens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sent_lens</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sentences</span> <span class="o">=</span> <span class="n">sentences</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sent_lens</span> <span class="o">=</span> <span class="n">sent_lens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_sent_lens</span> <span class="o">=</span> <span class="n">cum_sent_lens</span>
        <span class="k">return</span> <span class="n">sentences</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_reject_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;reject_prob&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">reject_prob</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reject_prob</span>

        <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_count</span>
        <span class="n">reject_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokens</span><span class="p">),))</span>
        <span class="n">n_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rev_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="n">reject_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">threshold</span> <span class="o">/</span> <span class="n">freq</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reject_prob</span> <span class="o">=</span> <span class="n">reject_prob</span>
        <span class="k">return</span> <span class="n">reject_prob</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_all_sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;all_sentences&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_sentences</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_sentences</span>

        <span class="n">sentences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sentences</span><span class="p">()</span>
        <span class="n">reject_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_reject_prob</span><span class="p">()</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_tokens</span><span class="p">()</span>
        <span class="n">all_sentences</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span>
                <span class="n">w</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span>
                <span class="k">if</span> <span class="mi">0</span> <span class="o">&gt;=</span> <span class="n">reject_prob</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">w</span><span class="p">]]</span>
                <span class="ow">or</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">reject_prob</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">w</span><span class="p">]]</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span> <span class="o">*</span> <span class="mi">30</span>
        <span class="p">]</span>
        <span class="n">all_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_sentences</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_sentences</span> <span class="o">=</span> <span class="n">all_sentences</span>
        <span class="k">return</span> <span class="n">all_sentences</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_random_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_all_sentences</span><span class="p">()</span>
        <span class="n">sent_id</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">sent</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span>
        <span class="n">word_id</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">sent</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">word_id</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span> <span class="p">:</span> <span class="n">word_id</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">word_id</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">):</span>
            <span class="n">context</span> <span class="o">+=</span> <span class="n">sent</span><span class="p">[</span><span class="n">word_id</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">),</span> <span class="n">word_id</span> <span class="o">+</span> <span class="n">C</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

        <span class="n">center</span> <span class="o">=</span> <span class="n">sent</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">center</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">center</span><span class="p">,</span> <span class="n">context</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_random_context</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">dataset_split</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;split&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">split</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">split</span>

        <span class="n">split</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">/datasetSplit.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">first</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">first</span><span class="p">:</span>
                    <span class="n">first</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">continue</span>
                <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
                <span class="n">split</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">split</span>
        <span class="k">return</span> <span class="n">split</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sampleTable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;sample_table&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_table</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_table</span>

        <span class="n">tokens_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">sampling_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tokens_num</span><span class="p">,))</span>

        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tokens_num</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rev_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_freq</span><span class="p">:</span>
                <span class="n">freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="n">freq</span> <span class="o">=</span> <span class="n">freq</span><span class="o">**</span><span class="mf">0.75</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">freq</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">sampling_freq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">sampling_freq</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampling_freq</span><span class="p">)</span>
        <span class="n">sampling_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sampling_freq</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sample_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">table_size</span><span class="p">),))</span>

        <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">table_size</span><span class="p">)):</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">sampling_freq</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
                <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_table</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_table</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_random_train_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">split</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_split</span><span class="p">()</span>
        <span class="n">sent_id</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_sentences</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_split_sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">split</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset_split</span><span class="p">()</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">all_sentences</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">split</span><span class="p">[</span><span class="n">split</span><span class="p">]]</span>
        <span class="k">return</span> <span class="n">sentences</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_train_sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_split_sentences</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_test_sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_split_sentences</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_val_sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_split_sentences</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sampleTokenIdx</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_table</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">StanfordSentiment</span><span class="p">()</span>  <span class="c1"># takes about 45sec</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">tokens</span>
<span class="n">num_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">num_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading">#</a></h2>
<p>As I mentioned in the motivation, we are trying to create a model that gives us a vector for each word that represents the meaning of that word. How could we do this?</p>
<p>It may not seem like it but the problem is of the shape: <span class="math notranslate nohighlight">\(y = Ax\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is our inputs, in our case the context words, <span class="math notranslate nohighlight">\(y\)</span> is our expected outputs, in our case the center word. Like I said, we are going to have our model predict a center word from the context words and then compare that prediction to the actual center word. Based on how close we were, we can then adjust the model so that it does a better job on another training example. This leaves two major questions:</p>
<ol class="arabic simple">
<li><p>How can we measure similarity between words mathematically?</p></li>
<li><p>How can we “adjust” our model? What does that even mean?</p></li>
</ol>
<section id="stochastic-gradient-descent">
<h3>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Link to this heading">#</a></h3>
<p>Luckily, there is a single process which will answer both of this questions.</p>
<blockquote>
<div><p>Before we discuss this topic, realize that no one just woke up one day and “discovered” this process. It took decades of mathematical and computational experimentation to develop. To that end, I do not expect you to <em>just</em> understand it, instead I want you to compile questions that you have. Pay close attention to what are you confused about and where you stop understanding.</p>
</div></blockquote>
<p>It is called Stochastic Gradient Descent or SGD and it will allow us both to create a quantitative similarity metric and to “learn” from what it tells us. As I posited above, this problem can be simiplied to the following: <span class="math notranslate nohighlight">\(y = Ax\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is our inputs, in our case the context words, <span class="math notranslate nohighlight">\(y\)</span> is our expected outputs, in our case the center word. In that case, what is <span class="math notranslate nohighlight">\(A\)</span>? <span class="math notranslate nohighlight">\(A\)</span> will be matrix of “weights” which when multiplied by our <span class="math notranslate nohighlight">\(x\)</span>s will product our <span class="math notranslate nohighlight">\(y\)</span>s.</p>
<p>We don’t know need to figure out <span class="math notranslate nohighlight">\(A\)</span>. It will start out as completely random and then we will learn its value through training. Importantly, each row in the <span class="math notranslate nohighlight">\(A\)</span> matrix is a single vector representing a word, so it will be as long as our entire vocabulary. So for each word in our text, we will have a initially random vector, whose size we will called <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> or embedding dimension.</p>
<p>Now that each word has a (random) vector associated with it, we can directly compare them. Using the <strong>the scaled dot product</strong>, we can determine how similar two vectors are. The scaled dot product (<span class="math notranslate nohighlight">\(x \cdot y\)</span>) between two vectors will produce a number between -1 and 1, representing how similar or dissimilar a vector is from any other. This answers our first question above.</p>
<p>Before we attack the second question, let’s first take a look at what we just described looks like in code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vector_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">word_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_words</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="n">vector_dim</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_words</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span>
        <span class="p">),</span>  <span class="c1"># for simplicity&#39;s sake, we will have a separate set of vectors for each context word as well as for each center word</span>
    <span class="p">),</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">word_vecs</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># 2*num_words (one for the context vector and another for the center vector) x vector_dim</span>
<span class="c1"># initially random vectors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># getting center word vecs and context word vecs</span>
<span class="c1"># each word will have two word vectors: center and context, we will only care about the center word vectors</span>
<span class="n">center_word_vecs</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[:</span><span class="n">num_words</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">outside_word_vecs</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[</span><span class="n">num_words</span><span class="p">:,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">center_word</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_random_context</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
<span class="n">center_word</span><span class="p">,</span> <span class="n">context</span>  <span class="c1"># get a center word and context</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># find index of center word</span>
<span class="n">center_word_idx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">center_word</span><span class="p">]</span>
<span class="n">center_word_idx</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># getting the random word vec for this index/word</span>
<span class="n">center_word_vec</span> <span class="o">=</span> <span class="n">center_word_vecs</span><span class="p">[</span><span class="n">center_word_idx</span><span class="p">]</span>
<span class="n">center_word_vec</span>  <span class="c1"># still random</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we are able to get the center word vector, we can get the vectors for the outside words in the same way. For each one, we are going to take the similarity (dot product) between it and and the center word vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example with just one outside word</span>
<span class="n">outside_word_idx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">outside_word_idx</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outside_word_vec</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span>
<span class="n">outside_word_vec</span>  <span class="c1"># start as zeros</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dot_products</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
    <span class="n">outside_word_vecs</span><span class="p">,</span> <span class="n">center_word_vec</span>
<span class="p">)</span>  <span class="c1"># take the dot product between all outside words and the center word</span>
<span class="n">dot_products</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s see what this dot product produces</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dot_products</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># it&#39;s all zeros because all of the outside word vectors are zero</span>
</pre></div>
</div>
</div>
</div>
<p>How can we take these numbers and get a prediction for word? Remember what we want to do: compare a predicted word to the actually correct center word. But right now all we have an array of zeros. How can we turn this into a prediction?</p>
<p>We will be using something called the <strong>softmax</strong> function, which is defined as: <span class="math notranslate nohighlight">\(\sigma(x_i) = \dfrac{e^{x_i}}{\sum_{j=1}^{K}e^{x_j}}\)</span>. This might look really scary, but don’t worry. All this function does is turn a set of numbers into a probability distribution. See below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">orig_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Matrix</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">/=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Vector</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">tmp</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">/=</span> <span class="n">tmp</span>

    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">orig_shape</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">softmax_probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">dot_products</span><span class="p">)</span>
<span class="n">softmax_probs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">softmax_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s see what this looks like</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">softmax_probs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># probabilities are even, at zero, as you might expect</span>
</pre></div>
</div>
</div>
</div>
<p>In this case, all of the words have the same probability: zero or almost zero (~5e-5), as softmax cannot output a value of zero. As a result, we can pick any word and compare it to our center word. To do so, we introduce a value called <em>loss</em> which represents how close we are to the true word for a given training example. There are many ways to calculate a loss, but because we are picking individual words, we are going to use <strong>negative log likelihood</strong>: <span class="math notranslate nohighlight">\(Loss = -y_{o,c}\ln(p_{o,c})\)</span>.</p>
<p>This also probably looks scary, but all it says is that we take the predicted word and the correct word and from their dot product can calculate a single number.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax_probs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">])</span>  <span class="c1"># nll in code</span>
<span class="n">loss</span>
<span class="c1"># this number represents how good our prediction is</span>
<span class="c1"># zero is the lowest number that we can predict</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent-continued">
<h3>Stochastic Gradient Descent continued<a class="headerlink" href="#stochastic-gradient-descent-continued" title="Link to this heading">#</a></h3>
<p>We have completed one half of our model! What we just did was called the <strong>forward pass</strong>. We are now going to investigate the <strong>backwards pass</strong>.</p>
<p>It is called a backward pass because we are going to go backwards through all of the steps in the forward pass to figure out what we need to do to make the prediction better. This process is also called <em>back propagation</em> or <em>backprop</em>.</p>
<p>So we need to figure out what elements of our <span class="math notranslate nohighlight">\(A\)</span> matrix (our word vectors) we need to change, and how to change them, so that we <strong>minimize our loss</strong>. This is an <em>optimization</em> problem, meaning we need to determine the <em>most optimal</em> values for each cell of <span class="math notranslate nohighlight">\(A\)</span> such that the loss between the predicted <span class="math notranslate nohighlight">\(y\)</span>s and the actual <span class="math notranslate nohighlight">\(y\)</span>s is the lowest we can make it. Thus, when our loss has reached the lowest it will go, we will have trained our word vectors so that they actually represent the meaning of their respective words. We will be able to show this by the end.</p>
<p>But how do we optimize? Well, it involves some calculus. We want to see how much we need to change each and every element of <span class="math notranslate nohighlight">\(A\)</span>, so we use a <em>derivative</em>, which will tell us how far away we are from reaching the lowest point in our loss function.</p>
<p>Thankfully, the derivatives we will need to calculate are fairly simple. All we have done is some multiplication and addition, which are very easy to take the derivative of. Don’t worry about this though, the derivatives will be provided below.</p>
<p>Once we have the derivative, we can take a small step in that direction by multiplying it by a small number (called a step size or learning rate) and subtract it from our values of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s see an example in code</span>
<span class="n">loss</span>  <span class="c1"># need to differential with respect to all of the values of A</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">current_grad_center_vec</span> <span class="o">=</span> <span class="o">-</span><span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
    <span class="n">softmax_probs</span><span class="p">,</span> <span class="n">outside_word_vecs</span>
<span class="p">)</span>  <span class="c1"># derivative of dot product for the center word vec</span>
<span class="n">current_grad_center_vec</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">current_grad_outside_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span>
    <span class="n">softmax_probs</span><span class="p">,</span> <span class="n">center_word_vec</span>
<span class="p">)</span>  <span class="c1"># derivative of dot product for the outer word vecs</span>
<span class="n">current_grad_outside_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span> <span class="o">-=</span> <span class="n">center_word_vec</span>
<span class="n">current_grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grad_center_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">center_word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># holder for our derivative values</span>
<span class="n">grad_outside_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
    <span class="n">outside_word_vecs</span><span class="o">.</span><span class="n">shape</span>
<span class="p">)</span>  <span class="c1"># holder for our derivative values</span>

<span class="n">grad_center_vecs</span><span class="p">[</span><span class="n">center_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">current_grad_center_vec</span>
<span class="n">grad_outside_vecs</span> <span class="o">+=</span> <span class="n">current_grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># now that we&#39;ve calculated our derivatives we can take a step</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">center_word_vecs</span> <span class="o">-=</span> <span class="n">step</span> <span class="o">*</span> <span class="n">grad_center_vecs</span>
<span class="n">outside_word_vecs</span> <span class="o">-=</span> <span class="n">step</span> <span class="o">*</span> <span class="n">grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># and then run another forward pass</span>
<span class="n">dot_products</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">outside_word_vecs</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
<span class="n">dot_products</span>  <span class="c1"># longer zero!</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">softmax_probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">dot_products</span><span class="p">)</span>
<span class="n">softmax_probs</span>  <span class="c1"># a bit more variation!</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax_probs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">])</span>
<span class="n">loss</span>  <span class="c1"># slight lower!!</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3>Negative Sampling<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>We just saw how using softmax and ggradient descent can reduce our loss, meaning that we can learn word meaning and represent that meaning with vectors! That’s great, but it takes too long. Softmax is a very expensive operation. We’ll use it in later lessons, but here, we’re going to use a similar technique, but a different activation function: <strong>sigmoid</strong>. Additionally, instead of applying softmax to every context word vector. We are only going to sample a small subset and estimate the loss based on that sample. This process is called <em>negative sampling</em>.</p>
<p>Sigmoid is defined as follows: <span class="math notranslate nohighlight">\(\frac{1}{1+e^{-x}}\)</span></p>
<p>Like softmax, sigmoid, transforms a set of numbers into probability distribution by not allowing any numbers greater than 1 or less than 0.</p>
<p>Let’s run through an example of negative sampling in code now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># new example copying from above</span>
<span class="n">vector_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">word_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_words</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="n">vector_dim</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_words</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span>
        <span class="p">),</span>  <span class="c1"># for simplicity&#39;s sake, we will have a separate set of vectors for each context word as well as for each center word</span>
    <span class="p">),</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">block_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">center_word</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_random_context</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>

<span class="n">center_word_idx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">center_word</span><span class="p">]</span>
<span class="n">center_word_vec</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[</span><span class="n">center_word_idx</span><span class="p">]</span>

<span class="n">outside_word_idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset</span><span class="o">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span><span class="p">]</span>

<span class="n">center_word_vecs</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[:</span><span class="n">num_words</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">outside_word_vecs</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[</span><span class="n">num_words</span><span class="p">:,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># first we need a data structure from which we can easily sample from</span>
<span class="c1"># we can use a table of values</span>
<span class="n">table_size</span> <span class="o">=</span> <span class="mf">1e8</span>


<span class="k">def</span><span class="w"> </span><span class="nf">sampleTable</span><span class="p">():</span>
    <span class="n">tokens_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">sampling_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tokens_num</span><span class="p">,))</span>

    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tokens_num</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">rev_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tok_freq</span><span class="p">:</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">tok_freq</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="n">freq</span><span class="o">**</span><span class="mf">0.75</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">freq</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">sampling_freq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">sampling_freq</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampling_freq</span><span class="p">)</span>
    <span class="n">sampling_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sampling_freq</span><span class="p">)</span> <span class="o">*</span> <span class="n">table_size</span>

    <span class="n">sample_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">table_size</span><span class="p">),))</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">table_size</span><span class="p">)):</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">sampling_freq</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">sample_table</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>

    <span class="k">return</span> <span class="n">sample_table</span>


<span class="n">sample_table</span> <span class="o">=</span> <span class="n">sampleTable</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample_table</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">table_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">negSampleWordIndices</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">newidx</span> <span class="o">=</span> <span class="n">sample_table</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">table_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">newidx</span><span class="p">)</span>
    <span class="n">negSampleWordIndices</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">newidx</span>
<span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">negSampleWordIndices</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># function version</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_negative_samples</span><span class="p">(</span><span class="n">outsideWordIdx</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="n">negSampleWordIndices</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">K</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">newidx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleTokenIdx</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">newidx</span> <span class="o">==</span> <span class="n">outsideWordIdx</span><span class="p">:</span>
            <span class="n">newidx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleTokenIdx</span><span class="p">()</span>
        <span class="n">negSampleWordIndices</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">newidx</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">negSampleWordIndices</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outside_word_idx</span> <span class="o">=</span> <span class="n">outside_word_idxs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">neg_samples</span> <span class="o">=</span> <span class="n">get_negative_samples</span><span class="p">(</span><span class="n">outside_word_idx</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">neg_samples</span>  <span class="c1"># neg samples</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grad_center_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">center_word_vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">grad_outside_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">outside_word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u_0</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span>
<span class="n">u_0</span>  <span class="c1"># vector for the true context word</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_0</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
<span class="n">z_0</span>  <span class="c1"># dot product as before</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="n">p_0</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_0</span><span class="p">)</span>
<span class="n">p_0</span>  <span class="c1"># new sigmoid transformation</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_0</span><span class="p">)</span>  <span class="c1"># loss for just this part</span>
<span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># derivatives for this part</span>
<span class="n">grad_center_vec</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p_0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">u_0</span>
<span class="n">grad_outside_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p_0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">center_word_vec</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">neg_samples</span><span class="p">:</span>  <span class="c1"># loop through neg sample idxs</span>
    <span class="n">u_k</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># find the correct context vector for a sample idx</span>
    <span class="n">z_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_k</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>  <span class="c1"># take the dot product as above</span>
    <span class="n">p_k</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">z_k</span><span class="p">)</span>  <span class="c1"># activate using sigmoid</span>
    <span class="n">loss</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_k</span><span class="p">)</span>  <span class="c1"># calculate the loss</span>

    <span class="c1"># derivatives for this negative sample</span>
    <span class="n">grad_center_vec</span> <span class="o">-=</span> <span class="p">(</span><span class="n">p_k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">u_k</span>
    <span class="n">grad_outside_vecs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">p_k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">center_word_vec</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span>  <span class="c1"># new loss about neg sampling</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example backwards pass</span>
<span class="n">step</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">center_word_vecs</span> <span class="o">-=</span> <span class="n">step</span> <span class="o">*</span> <span class="n">grad_center_vecs</span>
<span class="n">outside_word_vecs</span> <span class="o">-=</span> <span class="n">step</span> <span class="o">*</span> <span class="n">grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># checking if our grad descent worked like last time</span>
<span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">u_0</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span>
<span class="n">z_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_0</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
<span class="n">p_0</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">neg_samples</span><span class="p">:</span>
    <span class="n">u_k</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">z_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_k</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
    <span class="n">p_k</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">z_k</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_k</span><span class="p">)</span>

<span class="n">loss</span>  <span class="c1"># went down slightly!</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="running-the-model">
<h2>Running the model<a class="headerlink" href="#running-the-model" title="Link to this heading">#</a></h2>
<p>We have covered <em>A LOT</em> this lesson, so I have assembled the functions that we will need to train the model below. You have seen all of the code in them, though the presentation/order might be a little weird.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">w2v_wrapper</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">w2i</span><span class="p">,</span> <span class="n">word_vecs</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">loss_and_grad</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">center_word_vecs</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[:</span> <span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="p">:]</span>
    <span class="n">outside_word_vecs</span> <span class="o">=</span> <span class="n">word_vecs</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">block_size1</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>
        <span class="n">center_word</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_random_context</span><span class="p">(</span><span class="n">block_size1</span><span class="p">)</span>

        <span class="n">c</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
            <span class="n">center_word</span><span class="p">,</span>
            <span class="n">block_size1</span><span class="p">,</span>
            <span class="n">context</span><span class="p">,</span>
            <span class="n">w2i</span><span class="p">,</span>
            <span class="n">center_word_vecs</span><span class="p">,</span>
            <span class="n">outside_word_vecs</span><span class="p">,</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">loss_and_grad</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">c</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="n">grad</span><span class="p">[:</span> <span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">grad_in</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="n">grad</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">grad_out</span> <span class="o">/</span> <span class="n">batch_size</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the softmax function for each row of the input x.</span>
<span class="sd">    It is crucial that this function is optimized for speed because</span>
<span class="sd">    it will be used frequently in later code.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    x -- A D dimensional vector or N x D dimensional numpy matrix.</span>
<span class="sd">    Return:</span>
<span class="sd">    x -- You are allowed to modify x in-place</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">orig_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Matrix</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">/=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Vector</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">tmp</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">/=</span> <span class="n">tmp</span>

    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">orig_shape</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># normal softmax</span>
<span class="k">def</span><span class="w"> </span><span class="nf">softmaxloss_gradient</span><span class="p">(</span><span class="n">center_word_vec</span><span class="p">,</span> <span class="n">outside_word_idx</span><span class="p">,</span> <span class="n">outside_word_vecs</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="n">dot_products</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">outside_word_vecs</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
    <span class="n">softmax_probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">dot_products</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax_probs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">])</span>

    <span class="n">grad_center_vec</span> <span class="o">=</span> <span class="o">-</span><span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
        <span class="n">softmax_probs</span><span class="p">,</span> <span class="n">outside_word_vecs</span>
    <span class="p">)</span>
    <span class="n">grad_outside_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">softmax_probs</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
    <span class="n">grad_outside_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span> <span class="o">-=</span> <span class="n">center_word_vec</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad_center_vec</span><span class="p">,</span> <span class="n">grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_negative_samples</span><span class="p">(</span><span class="n">outsideWordIdx</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples K indexes which are not the outsideWordIdx&quot;&quot;&quot;</span>

    <span class="n">negSampleWordIndices</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">K</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">newidx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleTokenIdx</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">newidx</span> <span class="o">==</span> <span class="n">outsideWordIdx</span><span class="p">:</span>
            <span class="n">newidx</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sampleTokenIdx</span><span class="p">()</span>
        <span class="n">negSampleWordIndices</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">newidx</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">negSampleWordIndices</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># negative sampling</span>
<span class="k">def</span><span class="w"> </span><span class="nf">negative_samplingloss_gradient</span><span class="p">(</span>
    <span class="n">center_word_vec</span><span class="p">,</span> <span class="n">outside_word_idx</span><span class="p">,</span> <span class="n">outside_word_vecs</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">10</span>
<span class="p">):</span>
    <span class="n">neg_samples</span> <span class="o">=</span> <span class="n">get_negative_samples</span><span class="p">(</span><span class="n">outside_word_idx</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

    <span class="n">grad_center_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">center_word_vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">grad_outside_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">outside_word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">u_0</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span>
    <span class="n">z_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_0</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
    <span class="n">p_0</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_0</span><span class="p">)</span>

    <span class="n">grad_center_vec</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p_0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">u_0</span>
    <span class="n">grad_outside_vecs</span><span class="p">[</span><span class="n">outside_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p_0</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">center_word_vec</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">neg_samples</span><span class="p">:</span>
        <span class="n">u_k</span> <span class="o">=</span> <span class="n">outside_word_vecs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">z_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_k</span><span class="p">,</span> <span class="n">center_word_vec</span><span class="p">)</span>
        <span class="n">p_k</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">z_k</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_k</span><span class="p">)</span>

        <span class="n">grad_center_vec</span> <span class="o">-=</span> <span class="p">(</span><span class="n">p_k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">u_k</span>
        <span class="n">grad_outside_vecs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">p_k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">center_word_vec</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad_center_vec</span><span class="p">,</span> <span class="n">grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">skipgram</span><span class="p">(</span>
    <span class="n">current_center_word</span><span class="p">,</span>
    <span class="n">block_size1</span><span class="p">,</span>
    <span class="n">outside_words</span><span class="p">,</span>
    <span class="n">w2i</span><span class="p">,</span>
    <span class="n">center_word_vecs</span><span class="p">,</span>
    <span class="n">outside_word_vecs</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">loss_and_grad</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">grad_center_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">center_word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">grad_outside_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">outside_word_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">center_word_idx</span> <span class="o">=</span> <span class="n">w2i</span><span class="p">[</span><span class="n">current_center_word</span><span class="p">]</span>
    <span class="n">center_word_vec</span> <span class="o">=</span> <span class="n">center_word_vecs</span><span class="p">[</span><span class="n">center_word_idx</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">outside_word</span> <span class="ow">in</span> <span class="n">outside_words</span><span class="p">:</span>
        <span class="n">outside_word_idx</span> <span class="o">=</span> <span class="n">w2i</span><span class="p">[</span><span class="n">outside_word</span><span class="p">]</span>
        <span class="n">current_loss</span><span class="p">,</span> <span class="n">current_grad_center_vec</span><span class="p">,</span> <span class="n">current_grad_outside_vecs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">loss_and_grad</span><span class="p">(</span><span class="n">center_word_vec</span><span class="p">,</span> <span class="n">outside_word_idx</span><span class="p">,</span> <span class="n">outside_word_vecs</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">current_loss</span>
        <span class="n">grad_center_vecs</span><span class="p">[</span><span class="n">center_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">current_grad_center_vec</span>
        <span class="n">grad_outside_vecs</span> <span class="o">+=</span> <span class="n">current_grad_outside_vecs</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad_center_vecs</span><span class="p">,</span> <span class="n">grad_outside_vecs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">glob</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os.path</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">op</span>

<span class="n">SAVE_PARAMS_EVERY</span> <span class="o">=</span> <span class="mi">2000</span>


<span class="k">def</span><span class="w"> </span><span class="nf">load_saved_params</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A helper function that loads previously saved parameters and resets</span>
<span class="sd">    iteration start.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">st</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;saved_params_*.npy&quot;</span><span class="p">):</span>
        <span class="nb">iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">f</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">iter</span> <span class="o">&gt;</span> <span class="n">st</span><span class="p">:</span>
            <span class="n">st</span> <span class="o">=</span> <span class="nb">iter</span>

    <span class="k">if</span> <span class="n">st</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">params_file</span> <span class="o">=</span> <span class="s2">&quot;saved_params_</span><span class="si">%d</span><span class="s2">.npy&quot;</span> <span class="o">%</span> <span class="n">st</span>
        <span class="n">state_file</span> <span class="o">=</span> <span class="s2">&quot;saved_state_</span><span class="si">%d</span><span class="s2">.pickle&quot;</span> <span class="o">%</span> <span class="n">st</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">params_file</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">state_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">st</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">st</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">save_params</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">params_file</span> <span class="o">=</span> <span class="s2">&quot;saved_params_</span><span class="si">%d</span><span class="s2">.npy&quot;</span> <span class="o">%</span> <span class="nb">iter</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">params_file</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;saved_state_</span><span class="si">%d</span><span class="s2">.pickle&quot;</span> <span class="o">%</span> <span class="nb">iter</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">getstate</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>


<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">sgd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">use_saved</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">PRINT_EVERY</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ANNEAL_EVERY</span> <span class="o">=</span> <span class="mi">5000</span>
    <span class="k">if</span> <span class="n">use_saved</span><span class="p">:</span>
        <span class="n">start_iter</span><span class="p">,</span> <span class="n">oldx</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">load_saved_params</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">start_iter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">oldx</span>
            <span class="n">step</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">random</span><span class="o">.</span><span class="n">setstate</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_iter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">exploss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">gradient</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">step</span> <span class="o">*</span> <span class="n">gradient</span>

        <span class="k">if</span> <span class="n">exploss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">exploss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">exploss</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="o">*</span> <span class="n">exploss</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">loss</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">PRINT_EVERY</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">exploss</span><span class="p">:</span>
                <span class="n">exploss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">exploss</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="o">*</span> <span class="n">exploss</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">loss</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iter </span><span class="si">%d</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">exploss</span><span class="p">))</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">exploss</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">SAVE_PARAMS_EVERY</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">use_saved</span><span class="p">:</span>
            <span class="n">save_params</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">ANNEAL_EVERY</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">*=</span> <span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<section id="training-loop">
<h3>Training loop<a class="headerlink" href="#training-loop" title="Link to this heading">#</a></h3>
<p>Now that we have all of these functions and utilities we can finally put everything together and train a word2vec model. Training with these parameters will take about 20 minutes, so plan accordingly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">314</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">StanfordSentiment</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">tokens</span>
<span class="n">num_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">vector_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">31415</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9265</span><span class="p">)</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">word_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_words</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="n">vector_dim</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_words</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)),</span>
    <span class="p">),</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">word_vecs</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">vec</span><span class="p">:</span> <span class="n">w2v_wrapper</span><span class="p">(</span>
        <span class="n">skipgram</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">negative_samplingloss_gradient</span>
    <span class="p">),</span>
    <span class="n">word_vecs</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># was .01</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">use_saved</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">PRINT_EVERY</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;training took </span><span class="si">%d</span><span class="s2"> seconds&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h3>
<p>Now that our model is trained let’s make sure that our word vectors make sense and reflect underlying word meaning. these types of evaluations are difficult because word meaning is inherently qualitative, not quantitative. But we can still make some interpretations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">logging</span><span class="w"> </span><span class="kn">import</span> <span class="n">makeLogRecord</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss vs. Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">makeLogRecord</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trained_word_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span><span class="n">word_vecs</span><span class="p">[:</span><span class="n">num_words</span><span class="p">,</span> <span class="p">:],</span> <span class="n">word_vecs</span><span class="p">[</span><span class="n">num_words</span><span class="p">:,</span> <span class="p">:]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>  <span class="c1"># put all of center word vecs together</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize_words = [</span>
<span class="c1">#     &quot;great&quot;, &quot;cool&quot;, &quot;brilliant&quot;, &quot;wonderful&quot;, &quot;well&quot;, &quot;amazing&quot;,</span>
<span class="c1">#     &quot;worth&quot;, &quot;sweet&quot;, &quot;enjoyable&quot;, &quot;boring&quot;, &quot;bad&quot;, &quot;dumb&quot;,</span>
<span class="c1">#     &quot;annoying&quot;, &quot;female&quot;, &quot;male&quot;, &quot;queen&quot;, &quot;king&quot;, &quot;man&quot;, &quot;woman&quot;, &quot;rain&quot;, &quot;snow&quot;,</span>
<span class="c1">#     &quot;hail&quot;, &quot;coffee&quot;, &quot;tea&quot;]</span>

<span class="n">visualize_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Paris&quot;</span><span class="p">,</span> <span class="s2">&quot;London&quot;</span><span class="p">,</span> <span class="s2">&quot;England&quot;</span><span class="p">,</span> <span class="s2">&quot;France&quot;</span><span class="p">]</span>  <span class="c1"># analogies</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">visualize_words</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># dimension reduction for visualization</span>
<span class="n">visualize_vecs</span> <span class="o">=</span> <span class="n">trained_word_vectors</span><span class="p">[</span><span class="n">visualize_idx</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">visualize_vecs</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">visualize_vecs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">visualize_idx</span><span class="p">)</span> <span class="o">*</span> <span class="n">temp</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
<span class="n">coord</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">full_piece</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">visualize_words</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="n">coord</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">coord</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">visualize_words</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">coord</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">coord</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">coord</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">coord</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">makeLogRecord</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="conluding-remarks">
<h2>Conluding remarks<a class="headerlink" href="#conluding-remarks" title="Link to this heading">#</a></h2>
<p>Implementing anything from scratch as we did in the notebook is <strong>not</strong> easy. External libraries can be incredibly helpful, but challenging ourselves to not use them can give us a lot of insight into the inner workings of these libraries.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tokenizers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Byte-Pair Encoding Tokenization</p>
      </div>
    </a>
    <a class="right-next"
       href="w2v-gensim.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Natural Language Processing: Using Word Vectors for Textual Analysis</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item"><div
  id="pst-page-navigation-heading-2"
  class="page-toc tocsection onthispage"
>
  <i class="fa-solid fa-list"></i> word2vec using <code class="docutils literal notranslate"><span class="pre">numpy</span></code>
</div>
<nav
  class="bd-toc-nav page-toc"
  aria-labelledby="pst-page-navigation-heading-2"
>
  <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-our-training-dataset-collation">Creating our training dataset: Collation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-sampling">Negative Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#our-complete-dataset-object">Our complete dataset object</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-continued">Stochastic Gradient Descent continued</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Negative Sampling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-model">Running the model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conluding-remarks">Conluding remarks</a></li>
</ul>
</nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/TuftsRT/guides/edit/develop/source/nlp/deep-learning/w2v-from-scratch.ipynb">
      <i class="fa-solid fa-pencil"></i>
      
      
Suggest Edits 
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../../_sources/nlp/deep-learning/w2v-from-scratch.ipynb.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item"><div class="tocsection report-error">
  <a
    href="https://github.com/TuftsRT/guides/issues/new?title=Error Report: nlp/deep-learning/w2v-from-scratch"
    class="report-error-link"
    ><i class="fa-solid fa-triangle-exclamation"></i> Report Error
  </a>
</div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, Tufts University.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item"><p class="last-updated">
  Last updated on Nov 10, 2025.
  <br/>
</p></div>
      
    </div>
  
  
    <div class="footer-items__center">
      
        <div class="footer-item"><p class="disclaimer">
  Linked external resources not affiliated with or endorsed by Tufts University.
  <br />
</p></div>
      
        <div class="footer-item"><div class="footer-links">
  <ul>
    
    <li><a href="https://access.tufts.edu/digital-accessibility-policy">Accessibility</a></li>
    
    <li><a href="https://oeo.tufts.edu/policies-procedures/non-discrimination-statement/">Non-Discrimination</a></li>
    
    <li><a href="https://www.tufts.edu/about/privacy">Privacy</a></li>
    
  </ul>
</div></div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><div class="switcher-with-label">
  <div class="switcher-label">
    <p>Version:</p>
  </div>
  
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div>
</div></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>