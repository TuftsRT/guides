{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.gutenberg.org/files/25717/25717-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "\n",
    "In this workshop, we'll go over how to use regular expressions (AKA regex) for simple text processing. We'll practice these skills in Python but what you learn here can be applied to any regular expression system in any programming language.\n",
    "\n",
    "## What are regular expressions\n",
    "A regular expression is a string of characters that represent a *pattern*. The goal is the extract any substrings that *matches* a given pattern from a larger text.\n",
    "\n",
    "You can think of regular expressions as a mini-programming language. Just like in Python, you can type anything and call it regular expression, but it won't necessarily be valid and run. Regular expressions have a unique syntax and structure that must be practiced. That said, it is simpler than most programming languages so the goal of this notebook is to familiarize yourself with regular expressions so you can go on and learn more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "For this example, we'll be looking at Edward Gibbon's *Decline and Fall of the Roman Empire*. This is a useful example because we'll get a lot of results for general patterns and very few results for very specific patterns. There are also a lot of weird characters, so it simulates a difficult to work with dataset that you might encounter.\n",
    "\n",
    "First we'll see the basics of regular expressions and then we'll apply what we've learned to the task of currating this raw text into a usable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "with open(\"/content/25717-0.txt\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simplest patterns\n",
    "\n",
    "Let's start out with some very simple patterns to get used to how Python implements regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # regular expression library from PSL\n",
    "\n",
    "re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re` Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search\n",
    "re.search(\"the\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search\n",
    "re.search(\"the\", data).group(0), re.search(\"the\", data).start(), re.search(\n",
    "    \"the\", data\n",
    ").end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findall, similar to ctrl-f\n",
    "re.findall(\"the\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findall\n",
    "len(re.findall(\"the\", data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finditer\n",
    "re.finditer(\"the\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful, can split on 'the' in a word, like 'other'\n",
    "re.split(\"the\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub\n",
    "re.sub(\"the\", \"THE\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex patterns\n",
    "\n",
    "We've seen how we can use regular expressions like a the ctrl-f function to find a specific string in a larger string, but we can use the regular expression meta language to get more complex results.\n",
    "\n",
    "For instance, we can create a complex pattern to extract all of the elements of the table of contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to get all of the table of contents\n",
    "re.findall(\"\\n\\nChapter \", data)  # \\n means new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_list = re.findall(\"\\nChapter .*\\n\", data)\n",
    "chapter_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break that down:\n",
    "* `\\n` - new line character\n",
    "* `Chapter ` - arbitrary string to find (including the space)\n",
    "* `.` - match any character\n",
    "* `*` - *quantifier* that looking for the preceding pattern zero or more times, in this case modifying `.`\n",
    "* `.*` is a common idion in regex that means match any character until the next part of the pattern, in this case a `\\n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complex example: extracting the footnotes. Footnotes can be valuable resources of information, but they can also confuse downstream text analysis.\n",
    "\n",
    "This is what one from Gibbon lookings like:\n",
    ">60 (return) [ Vegetius finishes his second book, and the description of the legion, with the following emphatic words:—“Universa quæ in quoque belli genere necessaria esse creduntur, secum legio debet ubique portare, ut in quovis loco\n",
    "fixerit castra, armatam faciat civitatem.”]\n",
    "\n",
    "As you see, they start with a number, followed by the string `(return)` (this might look weird but it comes from the optical character recognitiion that got the text) and then the footnote in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first part\n",
    "re.findall(\"\\d+\\s\\(return\\)\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking it down:\n",
    "* `\\d` - matches any digit\n",
    "* `+` - *qunatifier* that matches the preceding character 1 or more times (very similar to `*`)\n",
    "* `\\s` - any whitespace (meaning spaces, tabs or new lines)\n",
    "* `\\(` and `\\)` - as we'll see in more detail later, `(` and `)` are special characters in regex, so we have to \"escape\" from them using the `\\` character\n",
    "* `return` - a specific string we're looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second part\n",
    "re.findall(\"\\d+\\s\\(return\\)\\s\\[.*\\]\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting results, but our example isn't there. That's because our pattern doesn't account for text that is on multiple lines. Let's break down what we have so far, though:\n",
    "* `\\d` - matches any digit\n",
    "* `+` - *qunatifier* that matches the preceding character 1 or more times (very similar to `*`)\n",
    "* `\\s` - matches any whitespace (meaning spaces, tabs or new lines)\n",
    "* `\\(` and `\\)` - as we'll see in more detail later, `(` and `)` are special characters in regex, so we have to \"escape\" from them using the `\\` character\n",
    "* `return` - a specific string we're looking for\n",
    "* `\\s` - matches any whitespace\n",
    "* `\\[` and `\\]` - similar to the parentheses, the `\\[` and `\\]` are special characters, so we have to \"escape\" from them using the `\\` character\n",
    "\n",
    "Unfortunately, `.*` does not match the `\\n` character, so we'll need to be creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the solution\n",
    "footnotes = re.findall(\"\\d+\\s\\(return\\)\\s\\[.*?\\]\", data, re.DOTALL)\n",
    "footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we only added two features:\n",
    "* In the parameters of `re.findall`, `re.DOTALL` - This *flag* will allows `.*` to match `\\n`\n",
    "* In the pattern, `?` - The question mark denotes is the *non-greedy* match, meaning the pattern `.*?` will match as few characters as possible\n",
    "\n",
    "We need both of these additions because while `re.DOTALL` allows us to match `\\n`, it will match all of the text between the first `\\[` and the last `\\]`. This leads to one big string that has the entire text from the beginning of the first footnote to the end of the last footnote. Obviously, this is not the behavior we want, so we have to use the `?` which will stop the matching after the first `\\]` it sees.\n",
    "\n",
    "\n",
    "Now we can finally clean up these footnotes to make them more readable, also using regex.\n",
    "* We'll extract the tet in the brackets\n",
    "* Then we'll remove any `\\n` and tabs (`\\t`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_fn = []\n",
    "for footnote in footnotes:\n",
    "    number = re.search(\"\\d+\", footnote).group()\n",
    "    text = re.search(\"\\[.*?\\]\", footnote, re.DOTALL).group(0)\n",
    "    text = re.sub(\"\\n|\\s{6}|\\[|\\]\", \"\", text).strip()\n",
    "    print(number, text)\n",
    "    clean_fn.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added some new symbols in the second pattern so let's take a closer look:\n",
    "* `|` - logical OR in regular expressions. In this case, we want to get rid of a bunch of types of characters, and this operator allows us to search for many different patterns at the same time\n",
    "* `{6}` - In the footnotes, there were strings of six spaces following a `\\n`. We can;t just remove all spaces because most of the spaces are needed to separate the words. Instead we can ask to remove only six consecutive spaces. The `{}` are a type of quantifier like `*` and `+` but we can specify a number of elements to expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text with regular expressions\n",
    "\n",
    "Now let's apply what we've learned to a specific use: cleaning this text so that it can be analyzed. We want to:\n",
    "\n",
    "* Remove any text added at the beginning or end by Project Gutenberg (licenses, disclaimers, etc...)\n",
    "\n",
    "* Remove the table of context or other reference material\n",
    "\n",
    "* Split the text into chapters\n",
    "\n",
    "* Store the accompanying footnotes in a separate data structure\n",
    "\n",
    "We can do all of this with what we've already learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title page\n",
    "daf_start = re.search(\"HISTORY OF THE DECLINE.*?\\(Revised\\)\", data, re.DOTALL).end()\n",
    "daf_end = re.search(\"\\n{5}\\*{3} END\", data).start()\n",
    "daf = data[daf_start:daf_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_list = re.findall(\"\\nChapter .*\\n\", data)\n",
    "chapter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(chapter_list[0].strip().split(\":\")[0], daf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_pattern = \"|\".join(set([f\"{c.strip().split(':')[0]}:\" for c in chapter_list]))\n",
    "chapter_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_split = re.split(chapter_pattern, daf)\n",
    "len(chapter_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\"\\n\\n      \", chapter_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_dict = {}\n",
    "chapter_split = chapter_split[1:]  # removing introduction\n",
    "for chapter in chapter_split:\n",
    "    # getting chapter text\n",
    "    title_match = re.search(\"\\n\\n\", chapter)\n",
    "    title_end = title_match.start()\n",
    "    text_start = title_match.end()\n",
    "    chapter_title = chapter[:title_end].replace(\"      \", \"\").replace(\"\\n\", \" \").strip()\n",
    "    chapter_text = chapter[text_start:].replace(\"      \", \"\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # separating and removing footnotes\n",
    "    footnotes = re.findall(\"\\d+.*?\\s\\(return\\)\\s\\[.*?\\]\", chapter_text, re.DOTALL)\n",
    "    chapter_text = re.sub(\"\\d+.*?\\s\\(return\\)\\s\\[.*?\\]\", \"\", chapter_text)\n",
    "    chapter_text = re.sub(\"\\d+\\s{4,6}\", \"\", chapter_text)\n",
    "    clean_fn = []\n",
    "    for footnote in footnotes:\n",
    "        number = re.search(\"\\d+\", footnote).group()\n",
    "        text = re.search(\"\\[.*?\\]\", footnote, re.DOTALL).group(0)\n",
    "        text = re.sub(\"\\n|\\s{4,7}|\\[|\\]\", \"\", text).strip()\n",
    "        clean_fn.append((number, text))\n",
    "\n",
    "    # load into dictionary\n",
    "    chapter_dict[chapter_title] = (chapter_text, clean_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chapter_dict.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = (\n",
    "    pd.DataFrame.from_dict(chapter_dict, orient=\"index\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"title\", 0: \"text\", 1: \"footnotes\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gibbon_chapters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some example analysis\n",
    "\n",
    "Below I'll show you the types of analysis we can achiever once we have cleaned our data. In this example, I use `gensim` to generate a word2vec model from the raw text we cleaned. To learn more, check out our forthcoming \"Working with `gensim`\" workshop.\n",
    "\n",
    "`gensim` allows us to convert the linguistic features of words into numerical objects called vectors. We can then manipulate these word vectors to analyze the text. It uses the context of the words around a given word to calculate these mathematical representations, thus data cleaning is very important. For instance, footnotes would skew and corrupt `gensim`, so we had to extract them and save them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim -Uq\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = list(df.text.apply(sent_tokenize).explode())\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "sentences = [[t for t in tokenize(sentence)] for sentence in sentences]\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences, vector_size=100, window=5, min_count=1, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gives us the words which are most similar to a given word\n",
    "model.wv.most_similar(\"Rome\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
