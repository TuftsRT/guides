{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets wiktionaryparser -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Hacking: Using `spaCy` for Morphosyntactic Analysis\n",
    "\n",
    "This workshop is inspired by [this article](https://hdsr.mitpress.mit.edu/pub/owxwohyz/release/6) written by Tufts professor, Dr. Gregory Crane. Crane teaches in the Classical Studies department, meaning that he studies historical languages like Ancient Greek and Latin, so you might be wondering how natural language processing could be relevant to such a discipline. The answer is in *language hacking*, the processing of taking a language which you may or may not know and using pretrained language models to give you a deeper understanding of the text.\n",
    "\n",
    "Professor Crane is a 'digital philologist.' Philology (from φιλολογία, the \"love of words\") is the study of language in historical sources, which can include everything from ancient literature to contemporary song lyrics. As a result, philologists are interested in many different languages, especially sources in many languages, but no one, no matter how good a philologist, can learn every language they might be interested in.\n",
    "\n",
    "This is where language hacking comes in. Deep learning models like the ones we'll play around with today offer new opportunities for research, language-learning and cross-cultural exchange.\n",
    "\n",
    "## What is `spaCy`?\n",
    "Above I mentioned that we would be using a language model to tell us about the meaning of words in languages we don't know. In this lesson, we'll download these models from a Python package called `spaCy`. `spaCy` is very powerful package with a lot of functionality. We'll only be using a small part of what they offer: their pretrained language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Model Preparation\n",
    "\n",
    "Before we can start language hacking, we need to set up our texts and models. For this example, I'll be using the original French version of Alexandre Dumas' *The Three Muskeeters* (*Les trois mousquetaires*). As a result, we'll also be using the French `spaCy` model, which we'll need to download.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pnadel/les_trois_mousquetaires\")\n",
    "data = dataset[\"train\"].to_pandas()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0][\"text\"][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading french model from spacy\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "nlp  # working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Model\n",
    "\n",
    "Now that we have our data and our model, we can apply the one to the other. The `nlp` object that we made above can be called like a function with some text. See below with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Morphosyntax\n",
    "\n",
    "This section serves as an introduction to some important NLP vocabulary and Python syntax for using `spaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is spacy `Doc` object\n",
    "example = nlp(\n",
    "    \"Je m'appelle Peter. J'aime les jeux vidéos.\"\n",
    ")  # \"My name is Peter. I like video games\"\n",
    "type(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .text just gives us the text as a string\n",
    "example.text, type(example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterable to access each sentence in the original text\n",
    "example.sents, type(example.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each sentence\n",
    "for sent in example.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each token\n",
    "for token in example:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each token\n",
    "# AND getting some morphosyntactic information\n",
    "for token in example:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_, sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See bleow for a glossary of relevant terms for the rest of this notebook.\n",
    "\n",
    "* *Lemma*: A lemma is the root form of a word. For example, the words \"ran\", \"running\" and \"runs\" all come from the root \"run\". In this case, \"run\" would be the lemma of \"ran\", \"running\" and \"runs\" (Accessed through the `lemma_` property).\n",
    "\n",
    "* *Part of speech*: You might be familiar with part of speech as the function that a word takes in a sentence, but there couple different standards for representing this information.\n",
    "\n",
    "    - *UPOS* or *Universal part of speech*: This is the \"normal\" part of speech that you likely saw while learning English (Accessed through the `pos_` property).\n",
    "\n",
    "    - *XPOS* or *Language-specific part of speech*: These are part of speech tags that might change depending on language. Oddly, they don't always change from language to language. In fact, they can be shared between languages but are often much more specific about the part of speech of a word (Accessed through the `tag_` property).\n",
    "\n",
    "* *Morphology*: To quote the `spaCy` docs, \"Inflectional morphology is the process by which a root form of a word is modified by adding prefixes or suffixes that specify its grammatical function but do not change its part-of-speech.\" So, where a lemma is the root form of a word, morphological features are what are added to the lemma to create grammatically correct variants of the same lemma (Accessed through the `morph` property).\n",
    "\n",
    "* *Sentence relation* or *Sentence dependency*: Each word, in addition to its part of speech and lemma, can be identified by what words it depends on or what words depend on it (or both). For example, the word \"apple\" is a `NOUN` yet it can be used either as a subject (\"The apple is red\") or an object (I ate the apple). The UPOS tag would be the same for each, but the sentence relation, that is what the word is doing in the sentence, would be different (Accessed through the `dep_` property). You can find a list of dependencies here: [English](https://universaldependencies.org/en/index.html) or [French](https://universaldependencies.org/fr/index.html).  \n",
    "\n",
    "* *Treebanks*: A sentence is made up of a series of words which are dependent on one another. This understanding allows us to construct tree-like structures of sentences. This is similar to diagraming sentences, if you have ever done that. It can be quite useful to use this model when language hacking, as it will give you a better idea about how to progress through a sentence. Below you can see the treebank that `spaCy` created for the first sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(list(example.sents)[0], style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in all treebanks, the verb (\"appelle\") depends on only one thing: root, a placeholder which represents the semantic beginning of the sentence. From here all word depend on the main verb of the sentence. Take a look at the next example to see a variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copulative example\n",
    "displacy.render(\n",
    "    nlp(\"Je suis fatigué\"), style=\"dep\", jupyter=True\n",
    ")  # \"I am tired\" in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this treebank, there is not word with the VERB tag. Instead, the root word is \"fatigué\" or \"tired\" in English. This is because the grammatical verb is \"suis\" or \"am\" in English is tagged as a `cop` or *copulative* (this is sometimes called a 'linking' verb in English education, as *copula* > Lat. *co-*, together and *apere*, fasten). These verbs only join a subject to an adjective but do not indicate any action. It is for this reason that in many language they are left out (cf. A. Gk. \"μακρός ὁ οἴκος\", meaning \"the house is large\", but literally \"the house large\"). Even in English, certain dialects like African American Vernacular English (AAVE) sometimes do not express copular verbs. For all of these reasons, they are marked differently than other verbs in treebanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Data\n",
    "\n",
    "Now that we have a grasp on the core vocabulary, we can begin to delve into real French literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chapter = data.iloc[0][\"text\"]\n",
    "first_chapter_doc = nlp(first_chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = list(first_chapter_doc.sents)[0]\n",
    "first_sentence.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In English: A short time ago, while making researches in the Royal Library for my History of Louis XIV., I stumbled by chance upon the Memoirs of M. d’Artagnan, printed—as were most of the works of that period, in which authors could not tell the truth without the risk of a residence, more or less long, in the Bastille—at Amsterdam, by Pierre Rogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(first_sentence, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree is very complex, so let's break it down using `spaCy`'s `head`, `rights`, `lefts` and `children` functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each token has a \"head\" word\n",
    "# this is the word that it depends on\n",
    "first_sentence[10], first_sentence[10].head\n",
    "# the head word of faisant is tombai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking out the root verb\n",
    "# looking for a token whose head is itself\n",
    "root = [token for token in first_sentence if token.head == token][0]\n",
    "root.text, root.dep_, root.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `lefts` returns a generator of all words to the left of a given token\n",
    "# which have that given token as their head\n",
    "print(\"LEFTS\")\n",
    "for t in root.lefts:\n",
    "    print(t.text, t.dep_, t.pos_, sep=\"\\t\\t\")\n",
    "\n",
    "print()\n",
    "# `rights` returns a generator of all words to the right of a given token\n",
    "# which have that given token as their head\n",
    "print(\"RIGHTS\")\n",
    "for t in root.rights:\n",
    "    print(t.text, t.dep_, t.pos_, sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `children` returns a generator of all of the descendants of a given word\n",
    "for descendant in root.children:\n",
    "    print(descendant.text, descendant.dep_, descendant.pos_, sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to Read in a Language We Don't Know\n",
    "\n",
    "We can now move on to using these morphosyntactic annotations to read text in a language we don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's apply the model to the whole text\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()  # for a progress bar\n",
    "\n",
    "data[\"spacy_docs\"] = data[\"text\"].progress_apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using wiktionary api as our dictionary\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "\n",
    "parser = WiktionaryParser()\n",
    "word = parser.fetch(\"hasard\", \"french\")\n",
    "word[0][\"definitions\"][0][\"text\"][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_definition(word):\n",
    "    parser = WiktionaryParser()\n",
    "    word = parser.fetch(word, \"french\")\n",
    "    try:\n",
    "        return word[0][\"definitions\"][0][\"text\"][1:]\n",
    "    except:\n",
    "        return \"No definition for this word. It is likely a proper noun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "example_sentence = list(data.iloc[-1].spacy_docs.sents)[\n",
    "    4\n",
    "]  # picking an easy example sentence, but feel free to alter the index to get a more complex sentence\n",
    "pp.pprint(example_sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treebank\n",
    "displacy.render(example_sentence, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to get started language hacking:\n",
    "\n",
    "1.   Find the root. This will usually be the verb.\n",
    "2.   Look at what directly depends on the root. Begin translation.\n",
    "3.   For each dependent word, look at what depends directly on it. Continue translation.\n",
    "4.   Make observations on word usage and syntax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find root and look it up\n",
    "root = [token for token in example_sentence if token.head == token][0]\n",
    "root.text, root.dep_, root.pos_, root.morph, look_up_definition(root.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this information, we can see that the root of tis sentences is \"entra\", that it means \"enter\" and that it is a 3rd person singular, indictative, past tense verb, which would translate to \"entered\" in English. Now we can look at the `lefts` and `rights` to find what depends on the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LEFTS: {[r for r in root.lefts]}\")\n",
    "first_dep = [r for r in root.lefts][0]\n",
    "first_dep.text, first_dep.dep_, first_dep.pos_, first_dep.morph, look_up_definition(\n",
    "    first_dep.lemma_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 'nsubj' tag, we can tell that this word is the subject of the verb \"entra\" and it means \"he\". In the context of the story, this \"he\" refers to King Louis XIII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RIGHTS: {[r for r in root.rights]}\")\n",
    "next_dep = [r for r in root.rights][0]\n",
    "next_dep.text, next_dep.dep_, next_dep.pos_, next_dep.morph, look_up_definition(\n",
    "    next_dep.lemma_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the \"obl:arg\" tag, we can see that the king entered some kind of suburb. Let's explore this word's dependencies to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for descendant in next_dep.children:\n",
    "    if descendant.dep_ != \"dep\":\n",
    "        print(\n",
    "            descendant.text,\n",
    "            descendant.dep_,\n",
    "            descendant.pos_,\n",
    "            look_up_definition(descendant.lemma_),\n",
    "            sep=\"\\t\\t\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this information, we can see that \"he [the king] entered by the the Saint-Jacques suburb.\" We're almost there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dep = [r for r in root.rights][1]\n",
    "last_dep.text, last_dep.dep_, last_dep.pos_, last_dep.morph, look_up_definition(\n",
    "    last_dep.lemma_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for descendant in last_dep.children:\n",
    "    if descendant.dep_ != \"dep\":\n",
    "        print(\n",
    "            descendant.text,\n",
    "            descendant.dep_,\n",
    "            descendant.pos_,\n",
    "            look_up_definition(descendant.lemma_),\n",
    "            sep=\"\\t\\t\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we have enough to create a translation of the whole sentence:\n",
    "\n",
    "**\"He [the king] entered by the Saint-Jacques suburb in a splendid ceremony.\"**\n",
    "\n",
    "Feel free to go back up and follow the same procedure with a different sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Language Hacking\n",
    "\n",
    "Language hacking is a very useful paradigm for reading text in languages you either don't know or are learning. It allows scholars to explore traditions and cultures that they would have been excluded from in the past. That said, it comes with some key limitations.\n",
    "\n",
    "* Dictionaries: As we saw above, we relied heavily on the open source wiktionary as our dictionary. This will work fine for a language like French with millions of speakers. But for languages that no one or very few people speak, a specialized dictionary will be necessary.\n",
    "\n",
    "* Available language models: The point above about dictionaries also holds for pretrained language models. `spaCy` has pretrained language models for a number of languages, but there are many, many more languages that they do not support. Training a new model for a new language is possible, but very time-consuming.\n",
    "\n",
    "* Inexact translations: Language hacking is by no means a substitute for a prepared translation. Translators can provide a much more competent rendering of the original language, but language hacking gives scholars an additional method to interrogate linguistic questions in texts from other languages.\n",
    "\n",
    "Please contact me at peter.nadel@tufts.edu for any questions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
