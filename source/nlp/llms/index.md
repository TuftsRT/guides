# Large Language Models

Amid the hype and fear around artificial intelligence, we have developed these resources to allow those interested to pursue rigorous exploration of the technology underlying systems like ChatGPT, Claude and Deepseek. These notebooks focus on small models available on HuggingFace, but the concepts can be applied to models of all sizes.

*Note on hardware*: Unlike the other notebooks in the NLP sequence, these notebooks require special hardware, graphical processing units or GPUs, to work. To that end, instead of displaying the notebook itself, there will be links to Google Colab notebooks, which allow users to run this code with a small GPU (Nvidia T4).

```{gallery-grid}
---
grid-columns: 1
---
- header: "{fas}`book` Information Retrieval Revisited"
  content: "Revisit information retrieval with pretrained models and compare various methods to each other."
  link: ""

- header: "{fas}`book` Using Pretrained LLMs from HuggingFace"
  content: "Learn the syntax for downloading and inferencing models from HuggingFace."
  link: ""

- header: "{fas}`book` Pretraining vs. Finetuning"
  content: "Explore three ways of training LLMs and observe the similarities and differences between them."
  link: ""

- header: "{fas}`book` Using LLM APIs"
  content: "Experience the pros and cons of using an LLM API rather than using models locally."
  link: ""
```
